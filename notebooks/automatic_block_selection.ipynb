{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6062989",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95e5daf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adbd75e",
   "metadata": {},
   "source": [
    "# Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff5fc535",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YahooFinance:\n",
    "    def __init__(self, symbol:str, start_date:str, end_date:str, interval:str='1d'):\n",
    "        self.symbol = symbol\n",
    "        self.start_date = start_date\n",
    "        self.end_date = end_date\n",
    "        self.interval = interval\n",
    "        self.col_names = [\"date\", 'price', 'log_return']\n",
    "    \n",
    "    def _download_data(self, symbol:str|list[str], start_date:str, end_date:str, interval:str='1d') -> pd.DataFrame:\n",
    "        prices = yf.download(symbol, start=start_date, end=end_date, interval=interval, auto_adjust=False)\n",
    "        prices.index = prices.index.date\n",
    "        return prices\n",
    "    \n",
    "    def _filter_cols(self, prices:pd.DataFrame) -> pd.DataFrame:\n",
    "        filtered_prices = prices['Adj Close'].copy()\n",
    "        filtered_prices.columns = ['Adj Close']\n",
    "        return filtered_prices\n",
    "    \n",
    "    def _pct_change(self, filtered_prices:pd.DataFrame) -> pd.DataFrame:\n",
    "        df = filtered_prices.copy()\n",
    "        df['log_return'] = np.log(df['Adj Close']/ df['Adj Close'].shift(1))\n",
    "        return df\n",
    "    \n",
    "    def _reset_index(self, filtered_prices:pd.DataFrame) -> pd.DataFrame:\n",
    "        df = filtered_prices.reset_index(names='Date', drop=False).copy()\n",
    "        df['Date'] = pd.to_datetime(df['Date']).dt.strftime('%d/%m/%Y')\n",
    "        df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%Y') #Convert from string to datetime\n",
    "        return df\n",
    "    \n",
    "    def pipeline(self) -> pd.DataFrame:\n",
    "        prices = self._download_data(self.symbol, self.start_date, self.end_date, self.interval)\n",
    "        filtered_prices = self._filter_cols(prices)\n",
    "        df = self._reset_index(filtered_prices)\n",
    "        df = self._pct_change(df)\n",
    "        df.columns = self.col_names\n",
    "        df.set_index('date', drop=True, inplace=True)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2e62257",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>log_return</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2000-01-03</th>\n",
       "      <td>91.617035</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-04</th>\n",
       "      <td>88.034309</td>\n",
       "      <td>-0.039891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-05</th>\n",
       "      <td>88.191727</td>\n",
       "      <td>0.001787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-06</th>\n",
       "      <td>86.774353</td>\n",
       "      <td>-0.016202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-07</th>\n",
       "      <td>91.813919</td>\n",
       "      <td>0.056453</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                price  log_return\n",
       "date                             \n",
       "2000-01-03  91.617035         NaN\n",
       "2000-01-04  88.034309   -0.039891\n",
       "2000-01-05  88.191727    0.001787\n",
       "2000-01-06  86.774353   -0.016202\n",
       "2000-01-07  91.813919    0.056453"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    "    'symbol': 'SPY',\n",
    "    'start_date': '2000-01-01',\n",
    "    'end_date': '2025-12-31'\n",
    "}\n",
    "yahoo = YahooFinance(**params)\n",
    "df = yahoo.pipeline()\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654d80ab",
   "metadata": {},
   "source": [
    "# Stationary Block Boostrapping (SBB)\n",
    "\n",
    "In order to realistically simulate the SPY returns, there are two important properties that must hold: (i) Convergence in Distribution and (ii) Volatility Clustering.\n",
    "\n",
    "Therefore, we will utilize the SBB due to the following properties:\n",
    "- **Local Stationarity Holds Over Short Horizons**: The bootstrap resamples short, contiguous blocks in which the distribution of returns is approximately stable. By stitching these blocks together, it preserves short-range dependence.\n",
    "- **Convergence in Distribution**: As $n \\to \\infty$ and $t \\to \\infty$ at a slower rate than $n$, the bootstrap distribution converges to the same asymptotic distribution as the original data.\n",
    "- **Volatility Clustering**: Financial returns exhibit autocorrelation and breaking structure also destroys this property. However, block bootstrap preserves short-range dependence within blocks and will be at least partially preserved.\n",
    "\n",
    "Nonetheless, there will be limitations:\n",
    "- **Finite-sample issue**: With short samples or poorly chosen block length, volatility dynamics may be distorted.\n",
    "- **Long-memory volatility**: Requires very large blocks, which reduces resample diversity.\n",
    "- **No new volatility shocks or unseen data**: Bootstrap cannot create what was not observed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe72d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stationary_bootstrap(returns:np.ndarray, n_sims:int=1, t:int=252, avg_block_size:int=None, seed:int=None) -> np.ndarray:\n",
    "    rng = np.random.default_rng(seed)\n",
    "    T = len(returns)\n",
    "    sims = np.zeros((n_sims, t))\n",
    "    if avg_block_size is None:\n",
    "        avg_block_size = int(np.floor(T**(1/3)))\n",
    "    \n",
    "    restart_prob = 1/avg_block_size\n",
    "    \n",
    "    for i in range(n_sims):\n",
    "        array_idx = 0\n",
    "        while array_idx < t:\n",
    "            if array_idx == 0 or rng.random() < restart_prob:\n",
    "                idx = rng.integers(T)\n",
    "            else:\n",
    "                idx = (idx + 1) % T #Implement circular indexing\n",
    "            \n",
    "            sims[i, array_idx] = returns[idx]\n",
    "            array_idx += 1\n",
    "    \n",
    "    return sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e06e551a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00113289,  0.00060227,  0.0038868 , ..., -0.00419623,\n",
       "         0.00760238, -0.0140082 ]], shape=(1, 5000))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_params = {\n",
    "    'returns': df['log_return'].dropna().to_numpy(),\n",
    "    'n_sims': 1,\n",
    "    't': 5000,\n",
    "    'avg_block_size': None,\n",
    "    'seed': 123\n",
    "}\n",
    "\n",
    "initial_bootstrap = stationary_bootstrap(**initial_params)\n",
    "initial_bootstrap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76edb86e",
   "metadata": {},
   "source": [
    "# Automatic Block Size Selection Methodology\n",
    "In block bootstrapping, increasing the block size reduces bias because larger blocks better preserve the original, long-range temporal dependence structure of the data. however, excessively large blocks reduce the number of independent resampling blocks, leading to higher variance in the bootstrap estimator. Consequently, block length selection involves a biasâ€“variance tradeoff, and the optimal block size is chosen to be the smallest value that adequately captures the dependence structure while minimizing variance.\n",
    "\n",
    "With that in mind and the fact that time-series data is typically dependent on previous values, we have the following notation for long-run variance:\n",
    "$$\\sigma^2_\\infty = \\gamma(0) + 2\\sum^\\infty_{k=1}\\gamma(k)$$\n",
    "where $\\gamma(k) = Cov[X_0, X_k]$ and where $k$ is the lag.\n",
    "\n",
    "## Adaptive Bandwidth Choice\n",
    "The best expected blocksize for the SBB is determined by minimizing the MSE as denoted:\n",
    "$$\n",
    "\\text{MSE}(\\hat{\\sigma}_b^2) = \n",
    "\\underbrace{\\text{Bias}^2}_{\\text{block too short}} + \n",
    "\\underbrace{\\text{Variance}}_{\\text{blocks too long}}\n",
    "$$\n",
    "\n",
    "Here, we can break it down into its components:\n",
    "$$Bias[\\hat{\\sigma}^2_b] = E[\\sigma^2_b] - \\sigma^2_{\\infty} = -\\frac{1}{b}G + o\\left(\\frac{1}{b}\\right)$$\n",
    "$$V[\\hat{\\sigma}^2_b] = \\frac{b}{N}D + o\\left(\\frac{b}{N}\\right)$$\n",
    "$$G = \\sum^\\infty_{k=-\\infty}|k|R(k) \\qquad D = 2g^2(0) \\qquad g(\\omega) = \\sum^\\infty_{s=-\\infty}R(s)cos(\\omega s)$$\n",
    "\n",
    "The above equations have some confusing notations, and hence we will clarify the intent and meaning of each:\n",
    "- $o\\left(\\frac{1}{b}\\right)$ and $o\\left(\\frac{b}{N}\\right)$ indicate that the remainder terms disappear faster than the primary terms as the sample size $N$ and block size $b$ grows.\n",
    "- $\\sigma^2_{\\infty}$ is the true asymptotic variance of the sample mean.\n",
    "- $D$ represents the variance constant for the SBB or how noisy each block-level contribution is.\n",
    "- $g(\\omega)$ is the spectral density function, if your process has large low-frequency power (strong persistence / volatility clustering), every block carries a lot of correlated noise.\n",
    "- $R(k)$ or $R(s)$ is simply a different notation for $\\gamma(k)$ and $\\gamma(s)$, they have equivalent meaning.\n",
    "\n",
    "Lastly, $G$ is harder to understand by itself. It is easier to understand if we look at how it affects the Bias term.\n",
    "$$Bias[\\hat{\\sigma}^2_b]= -\\frac{1}{b}G + o\\left(\\frac{1}{b}\\right)$$\n",
    " \n",
    "The negative sign is included because the SBB cuts the time series into chunks, destroying some correlation and that means less long-run variance, leading to an estimate that is bias downwards. Moreover, at lag $k$, dependence survive with probability $1 - \\frac{|k|}{b}$ and broken with probability $\\frac{|k|}{b}$. Naturally, this leads to the following weighted-sum equation:\n",
    "$$Bias[\\hat{\\sigma}^2_b] \\approx -\\sum^\\infty_{k=-\\infty}\\frac{|k|}{b}R(k)$$\n",
    "$$Bias[\\hat{\\sigma}^2_b] \\approx -\\frac{1}{b}G$$\n",
    "\n",
    "This leads us to the final MSE equation:\n",
    "$$\\text{MSE}(\\hat{\\sigma}_b^2) = \\frac{G^2}{b^2} + D\\frac{b}{N} + o(b^{-2}) + o(b/N)$$\n",
    "\n",
    "If we want to minimize MSE, we would choose:\n",
    "$$b = \\left(\\frac{2G^2}{D}\\right)^{1/3} N^{1/3}$$\n",
    "$$\\text{MSE}(\\hat{\\sigma}_b^2) \\approx \\frac{3}{2^{2/3}}\\frac{G^{2/3}D^{2/3}}{N^{2/3}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ad0cb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76c0403d",
   "metadata": {},
   "source": [
    "## Flat-Top Lag-Window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c19e78",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
